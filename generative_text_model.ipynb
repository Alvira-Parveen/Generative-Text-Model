{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f71a70f3-8a7d-4d7f-929d-793668d60d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline, set_seed\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89676648-14f3-4272-a9b4-91aad4469af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text 1:\n",
      " The future of Artificial Intelligence in education is still in its infancy, but with a lot of work to do, we can expect the future of AI to be much more complex.\n",
      "\n",
      "You can read the full article here.\n",
      "\n",
      "Generated Text 2:\n",
      " The future of Artificial Intelligence in education is far from clear. The question is whether an AI may or may not be an intelligent person. This is not so much a question of whether an AI could be a human for reasons of convenience, as whether an AI has the ability to understand and understand human language (or speech). This has not yet been demonstrated. However, there is little doubt that a human is capable of understanding and understanding human language.\n",
      "\n",
      "This is not all, though. Most likely, AI will be used in everyday tasks, and will probably be useful to a variety of different people in different capacities. It is not clear yet why, for example, humans will prefer to use computers to do tasks as complex as writing a book, or why they may not like to use robots to search for objects. The questions to be considered are simply the human-to-human interaction or interaction between human and AI.\n",
      "\n",
      "Perhaps the most important question is whether AI will be used to work with or against certain types of people. If so, the human-to-human interaction is likely to be limited to individuals who are not the kinds of people that a human would be able to interact with. However, this is certainly not a new approach to AI. In fact, research has shown that most people would\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "set_seed(42)\n",
    "\n",
    "prompt = \"The future of Artificial Intelligence in education is\"\n",
    "results = generator(prompt, max_length=80, num_return_sequences=2)\n",
    "\n",
    "for i, r in enumerate(results):\n",
    "    print(f\"\\nGenerated Text {i+1}:\\n\", r['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "290dcb51-79c2-4cf2-af0a-4bab085d327f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative models models trained model for for text text generation paragraphs paragraphs text by datasets powerful to generative one next creation new words generation new applications creation transforming tools understand words are\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# Training text\n",
    "# ----------------------------\n",
    "text = (\n",
    "    \"Generative models are powerful tools for creating text. \"\n",
    "    \"They can learn patterns from a dataset and generate new content. \"\n",
    "    \"LSTM networks are one type of model used in text generation. \"\n",
    "    \"Text generation has applications in chatbots, story writing, and content creation. \"\n",
    "    \"Machine learning allows computers to understand sequences and predict next words. \"\n",
    "    \"Training on more data improves the quality of generated text. \"\n",
    "    \"Using word-level tokenization produces more readable sentences. \"\n",
    "    \"AI is transforming education, healthcare, and creative industries. \"\n",
    "    \"Models like GPT-2 are trained on massive datasets to generate coherent paragraphs. \"\n",
    "    \"This demo shows how a small LSTM can generate text word by word.\"\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Tokenize words\n",
    "# ----------------------------\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# ----------------------------\n",
    "# Create sequences\n",
    "# ----------------------------\n",
    "input_sequences = []\n",
    "for line in text.split('. '):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Pad sequences\n",
    "max_seq_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre'))\n",
    "\n",
    "# Split predictors and label\n",
    "X = input_sequences[:, :-1]\n",
    "y = tf.keras.utils.to_categorical(input_sequences[:, -1], num_classes=total_words)\n",
    "\n",
    "# ----------------------------\n",
    "# Build model\n",
    "# ----------------------------\n",
    "model = Sequential([\n",
    "    layers.Embedding(total_words, 32, input_length=max_seq_len-1),\n",
    "    layers.LSTM(64),\n",
    "    layers.Dense(total_words, activation='softmax')\n",
    "])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# ----------------------------\n",
    "# Train model\n",
    "# ----------------------------\n",
    "model.fit(X, y, epochs=100, verbose=0)\n",
    "\n",
    "# ----------------------------\n",
    "# Text generation with temperature\n",
    "# ----------------------------\n",
    "def generate_text(seed_text, next_words=30, temperature=0.7):\n",
    "    result = seed_text\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([result])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n",
    "        preds = model.predict(token_list, verbose=0)[0]\n",
    "        \n",
    "        # Apply temperature sampling\n",
    "        preds = np.log(preds + 1e-8) / temperature\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "        \n",
    "        next_index = np.random.choice(len(preds), p=preds)\n",
    "        output_word = tokenizer.index_word.get(next_index, '')\n",
    "        result += ' ' + output_word\n",
    "    return result\n",
    "\n",
    "# ----------------------------\n",
    "# Generate sample text\n",
    "# ----------------------------\n",
    "print(generate_text(\"Generative models\", next_words=30, temperature=0.7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b45d0b-b04b-4210-bb2b-9ac7f9866aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
